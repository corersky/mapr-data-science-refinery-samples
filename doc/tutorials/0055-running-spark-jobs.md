# Running Spark Jobs in Zeppelin

The Zeppelin notebooks for this section shows how to run Apache Spark jobs using either the [Livy](https://mapr.com/docs/61/Zeppelin/ConfigureLivyInterpreter.html#task_t1d_4yj_qbb) or [Spark](https://mapr.com/docs/61/Zeppelin/ConfigureSparkInterpreter.html#task_t1d_4yj_qbb) interpreter.

> Note: The examples in this section use Hadoop commands to access files in MapR Filesystem. If you have a MapR Filesystem mount point in your container, you can replace the Hadoop commands with standard shell commands. Refer to Running Shell Commands in Zeppelin for an example of how to do this.

To run the notebook [Running Spark Jobs in Zeppelin](notebook/running-spark-jobs-in-zeppelin.json) or [Running Spark SQL Jobs in Zeppelin](notebook/running-spark-sql-jobs-in-zeppelin.json)just import it to the Zeppelin, click on  `Import note:` button and select the JSON file or put the link to the notebook:

![import Zeppelin notebook](images/zeppelin-import.png)
